# Pancreas-detection-YOLOv4


Pancreas detection using YOLOv4 model. The model achieved a 71.43% mAP on the NIH testing set.

The model was trained and tested using the NIH [1], [2], [3] and Decathlon [4] datasets.

On the following figure is visualized the detection of pancreas on a NIH (a) and Decathlon (b) slice, respectively. The green color represents the ground-truth and the purple the YOLO prediction.

![successful_pancreas_detection](https://user-images.githubusercontent.com/30274421/111903314-631d2700-8a4a-11eb-9beb-ffcdfd85c2a4.png)



[1] H. R. Roth, A. Farag, E. B. Turkbey, L. Lu, J. Liu, and R. M. Summers, “Data from pancreas-ct.” The Cancer Imaging Archive, 2016. [Online]. Available: https://doi.org/10.7937/K9/TCIA.2016.TNB1kqBU 

[2] H. R. Roth, L. Lu, A. Farag, H.-C. Shin, J. Liu, E. B. Turkbey, and R. M. Summers, “Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation,” in International conference on medical image computing and computer-assisted intervention. Springer, 2015, pp. 556–564.

[3] K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore, S. Phillips, D. Maffitt,
M. Pringle et al., “The cancer imaging archive (tcia): maintaining and operating a public information repository,” Journal of digital imaging, vol. 26, no. 6, pp. 1045–1057, 2013. 

[4] B. M. Dawant, R. Li, B. Lennon, and S. Li, “Semi-automatic segmentation of the liver and its
evaluation on the miccai 2007 grand challenge data set,” 3D Segmentation in The Clinic: A Grand
Challenge, pp. 215–221, 2007.
